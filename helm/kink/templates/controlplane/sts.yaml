{{- $k8sBin := "k3s" }}
{{- $dataDir := "/var/lib/rancher/k3s" }}
{{- $tokenVar := "K3S_TOKEN" }}
{{- $extraManifestsDir := "k3s" }}
{{- if .Values.rke2.enabled }}
{{- $k8sBin = "rke2" }}
{{- $dataDir = "/var/lib/rancher/rke2" }}
{{- $tokenVar = "RKE2_TOKEN" }}
{{- $extraManifestsDir = "rke2" }}
{{- end }}
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ include "kink.controlplane.fullname" . }}
  labels:
    {{- include "kink.controlplane.labels" . | nindent 4 }}
spec:
  serviceName: {{ include "kink.controlplane.fullname" . }}
  replicas: {{ .Values.controlplane.replicaCount }}
  selector:
    matchLabels:
      {{- include "kink.controlplane.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.controlplane.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "kink.controlplane.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "kink.controlplane.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.controlplane.podSecurityContext | nindent 8 }}
      initContainers:
        - name: init
          securityContext:
            {{- toYaml .Values.controlplane.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          env:
          {{- if gt (int .Values.controlplane.replicaCount) 1 }}
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: ETCD_CONFIG_FILE
            value: '{{ $dataDir }}/server/db/etcd/config'
          - name: ETCD_NAME_FILE
            value: '{{ $dataDir }}/server/db/etcd/name'
          {{- end }}
          {{- with .Values.extraEnv }}
          {{- . | toYaml | nindent 10 }}
          {{- end }}
          {{- with .Values.controlplane.extraEnv }}
          {{- . | toYaml | nindent 10 }}
          {{- end }}
          command: [sh, -cxe]
          args:
          - |-
              {{- if gt (int .Values.controlplane.replicaCount) 1 }}
              if [ -f "${ETCD_CONFIG_FILE}" ]; then
                echo 'Etcd config file present, attempting to update own member IP'
                export ETCDCTL_CACERT=$(yq '.["client-transport-security"].["trusted-ca-file"]' "${ETCD_CONFIG_FILE}")
                export ETCDCTL_CERT=$(yq '.["client-transport-security"].["cert-file"]' "${ETCD_CONFIG_FILE}")
                export ETCDCTL_KEY=$(yq '.["client-transport-security"].["key-file"]' "${ETCD_CONFIG_FILE}")
                export ETCDCTL_ENDPOINTS='https://{{ include "kink.controlplane.fullname" . }}:2379'
                ETCD_NAME=$(cat "${ETCD_NAME_FILE}")
                ETCD_ID=$(etcdctl member list --write-out=json | tee /dev/stderr | yq '.members[] | tee /dev/stderr | select(.name|test("'"${POD_NAME}"'")).ID')
                if [ -z "${ETCD_ID}" ]; then
                  echo "This node is not an etcd member, not updating"
                else
                  ETCD_ID=$(printf '%x' "${ETCD_ID}")
                  etcdctl member update "${ETCD_ID}" --peer-urls="https://${POD_IP}:2380"
                fi
              else
                echo "No etcd config file present, assuming this node isn't initialized"
              fi
              {{- end }}
              echo 'Copying extra manifests'
              mkdir -p '{{ $dataDir }}/server/manifests/'
              {{- if .Values.rke2.enabled }}
              cp /etc/kink/extra-manifests/{{ $extraManifestsDir }}/system/kink-local-path-provisioner.yaml '{{ $dataDir }}/server/manifests/'
              {{- if .Values.sharedPersistence.enabled }}
              cp /etc/kink/extra-manifests/{{ $extraManifestsDir }}/system/kink-shared-local-path-provisioner.yaml '{{ $dataDir }}/server/manifests/'
              {{- end }}
              {{- else }}
              {{- if .Values.sharedPersistence.enabled }}
              cp /etc/kink/extra-manifests/{{ $extraManifestsDir }}/system/kink-shared-local-path-provisioner.yaml '{{ $dataDir }}/server/manifests/'
              {{- end }}
              {{- end }}
              # TODO: This will always succeed, even if a copy fails, fix this

              find '/etc/kink/extra-manifests/{{ $extraManifestsDir }}/user/' -name '*.yaml' -exec cp {} '{{ $dataDir }}/server/manifests/' \;
          resources:
            {{- toYaml .Values.controlplane.resources | nindent 12 }}
          volumeMounts:
          - name: data
            mountPath: '{{ $dataDir }}'
            subPath: '{{ $dataDir | trimPrefix "/" }}'
          - name: data
            mountPath: /etc/rancher
            subPath: etc/rancher
          - name: kubelet
            mountPath: /var/lib/kubelet
            subPath: var/lib/rancher
          {{- range .Values.controlplane.persistence.extraMounts }}
          - name: data
            mountPath: /{{ . }}
            subPath: {{ . | trimPrefix "/" }}
          {{- end }}
          {{- with .Values.extraVolumeMounts }}
          {{- . | toYaml | nindent 10 }}
          {{- end }}
          {{- with .Values.controlplane.extraVolumeMounts }}
          {{- . | toYaml | nindent 10 }}
          {{- end }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.controlplane.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: '{{ $tokenVar }}'
            valueFrom:
              secretKeyRef:
                {{- if .Values.token.existingSecret.name }}
                name: {{ .Values.token.existingSecret.name }}
                key:  {{ .Values.token.existingSecret.key }}
                {{- else }}
                name: {{ include "kink.fullname" . }}
                key: token
                {{- end }}
          {{- with .Values.extraEnv }}
          {{- . | toYaml | nindent 10 }}
          {{- end }}
          {{- with .Values.controlplane.extraEnv }}
          {{- . | toYaml | nindent 10 }}
          {{- end }}
          command:
          - sh
          - -ce
          - |
            {{- if .Values.iptables.useLegacy }}
            update-alternatives --set iptables /usr/sbin/iptables-legacy
            update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy
            {{- end }}
            {{- if gt (int .Values.controlplane.replicaCount) 1 }}
            if [ "${POD_NAME}" = '{{ include "kink.controlplane.fullname" . }}-0' ]; then
              exec '{{ $k8sBin }}' server \
                {{- if not .Values.rke2.enabled }}
                --cluster-init \
                {{- end }}
                "$0" \
                "$@"
            else
              exec '{{ $k8sBin }}' server \
                --server='{{ include "kink.controlplane.url" . }}' \
                "$0" \
                "$@"
            fi
            {{- else }}
            exec '{{ $k8sBin }}' server "$0" "$@"
            {{- end }}
          args:
          - '--data-dir={{ $dataDir }}'
          - '--tls-san={{ .Release.Name }}'
          - '--tls-san={{ include "kink.controlplane.fullname" . }}'
          - '--tls-san={{ include "kink.controlplane.fullname" . }}.{{ .Release.Namespace }}'
          - '--tls-san={{ include "kink.controlplane.fullname" . }}.{{ .Release.Namespace }}.svc'
          - '--tls-san={{ include "kink.controlplane.fullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}'
          - '--tls-san=$(POD_NAME).{{ include "kink.controlplane.fullname" . }}.{{ .Release.Namespace }}.svc.{{ .Values.clusterDomain }}'
          {{- if .Values.controlplane.ingress.enabled }}
          {{- range .Values.controlplane.ingress.hosts }}
          - '--tls-san={{ . }}'
          {{- end }}
          {{- end }}
          {{- if .Values.controlplane.defaultTaint }}
          - --node-taint=node-role.kubernetes.io/control-plane=true:NoSchedule
          {{- end }}
          {{- range $taint := .Values.controlplane.extraTaints }}
          - --node-taint='{{ $taint.key }}={{ $taint.value }}:{{ $taint.effect }}'
          {{- end }}
          {{- with .Values.extraArgs }}
          {{- . | toYaml | nindent 10 }}
          {{- end }}
          {{- with .Values.controlplane.extraArgs }}
          {{- . | toYaml | nindent 10 }}
          {{- end }}
          ports:
            - name: api
              containerPort: 6443
              protocol: TCP
            - name: kubelet-metrics
              containerPort: 10250
              protocol: TCP
            - name: etcd-client
              containerPort: 2379
              protocol: TCP
            - name: etcd-peer
              containerPort: 2380
              protocol: TCP
            {{- if .Values.rke2.enabled }}
            - name: rke2-discover
              containerPort: 9345
              protocol: TCP
            {{- end }}

          startupProbe:
            tcpSocket:
              port: api
            # TODO: Health checks w/ authentication
            # httpGet:
            #   path: /livez
            #   port: api
            #   scheme: HTTPS
            # exec:
            #   command:
            #   - k3s
            #   - kubectl
            #   - --kubeconfig=/etc/rancher/k3s/k3s.yaml
            #   - version
            failureThreshold: 20
            periodSeconds: 15
          livenessProbe:
            # httpGet:
            #   path: /livez
            #   scheme: HTTPS
            #   port: api
            # exec:
            #  command:
            #  - k3s
            #  - kubectl
            #  - --kubeconfig=/etc/rancher/k3s/k3s.yaml
            #  - version
            tcpSocket:
              port: api
            # exec:
            #   command:
            #   - curl
            #   - -fv
            #   - --cacert=/var/lib/rancher/k3s/server/tls/server-ca.crt
            #   - --cert /var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt
            #   - --key /var/lib/rancher/k3s/server/tls/client-kubapiserver.key
            #   - https://localhost:6443/livez

          readinessProbe:
            #httpGet:
            #  path: /readyz
            #  scheme: HTTPS
            #  port: api
                #  exec:
                # command:
                # - k3s
                # - kubectl
                # - --kubeconfig=/etc/rancher/k3s/k3s.yaml
                # - version
            tcpSocket:
              port: api
            # exec:
            #   command:
            #   - curl
            #   - -fv
            #   - --cacert=/var/lib/rancher/k3s/server/tls/server-ca.crt
            #   - --cert /var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt
            #   - --key /var/lib/rancher/k3s/server/tls/client-kubapiserver.key
            #   - https://localhost:6443/readyz
          resources:
            {{- toYaml .Values.controlplane.resources | nindent 12 }}
          volumeMounts:
          - name: data
            mountPath: '{{ $dataDir }}'
            subPath: '{{ $dataDir | trimPrefix "/" }}'
          - name: data
            mountPath: /etc/rancher
            subPath: etc/rancher
          - name: kubelet
            mountPath: /var/lib/kubelet
            subPath: var/lib/rancher
          {{- range .Values.controlplane.persistence.extraMounts }}
          - name: data
            mountPath: /{{ . }}
            subPath: {{ . | trimPrefix "/" }}
          {{- end }}
          {{- if .Values.sharedPersistence.enabled }}
          {{- range .Values.sharedPersistence.mounts }}
          - name: shared-data
            mountPath: /{{ . }}
            subPath: {{ . | trimPrefix "/" }}
          {{- end }}
          {{- end }}
          {{- with .Values.extraVolumeMounts }}
          {{- . | toYaml | nindent 10 }}
          {{- end }}
          {{- with .Values.controlplane.extraVolumeMounts }}
          {{- . | toYaml | nindent 10 }}
          {{- end }}

      {{- with .Values.controlplane.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.controlplane.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.controlplane.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      volumes:
      {{- if not .Values.kubelet.persistence.enabled }}
      - name: kubelet
        emptyDir: {}
      {{- end }}
      {{- if not .Values.controlplane.persistence.enabled }}
      - name: data
        emptyDir: {}
      {{- end }}
      {{- if .Values.sharedPersistence.enabled }}
      - name: shared-data
        persistentVolumeClaim:
          claimName: {{ include "kink.fullname" . }}-shared
      {{- end }}
      {{- with .Values.extraVolumes }}
      {{- . | toYaml | nindent 6 }}
      {{- end }}
      {{- with .Values.controlplane.extraVolumes }}
      {{- . | toYaml | nindent 6 }}
      {{- end }}
  volumeClaimTemplates:
  {{- if .Values.controlplane.persistence.enabled }}
  - metadata:
      name: data
    spec:
      accessModes: {{ .Values.controlplane.persistence.accessModes | toJson }}
      {{- with .Values.controlplane.persistence.storageClassName }}
      storageClassName: '{{ . }}'
      {{- end }}
      resources:
        requests:
          storage: {{ .Values.controlplane.persistence.size }}
  {{- end }}
  {{- if .Values.kubelet.persistence.enabled }}
  - metadata:
      name: kubelet
    spec:
      accessModes: {{ .Values.kubelet.persistence.accessModes | toJson }}
      {{- with .Values.kubelet.persistence.storageClassName }}
      storageClassName: '{{ . }}'
      {{- end }}
      resources:
        requests:
          storage: {{ .Values.kubelet.persistence.size }}
  {{- end }}

